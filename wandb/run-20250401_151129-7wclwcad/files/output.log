2025-04-01 15:11:29.905 | INFO     | __main__:main:231 - Using dist with rank 0 (only rank 0 will log)
2025-04-01 15:11:29.905 | INFO     | __main__:main:232 - ****************************************
2025-04-01 15:11:29.905 | INFO     | __main__:main:233 - Starting training with the arguments
2025-04-01 15:11:29.905 | INFO     | __main__:main:235 - model_type                     moc
2025-04-01 15:11:29.906 | INFO     | __main__:main:235 - run_name                       moc_60m-LR-0.006
2025-04-01 15:11:29.906 | INFO     | __main__:main:235 - wandb_project                  moc_pretrain
2025-04-01 15:11:29.906 | INFO     | __main__:main:235 - model_config                   configs/llama/llama_60m.json
2025-04-01 15:11:29.907 | INFO     | __main__:main:235 - continue_from                  None
2025-04-01 15:11:29.907 | INFO     | __main__:main:235 - batch_size                     256
2025-04-01 15:11:29.907 | INFO     | __main__:main:235 - gradient_accumulation          2
2025-04-01 15:11:29.907 | INFO     | __main__:main:235 - total_batch_size               512
2025-04-01 15:11:29.907 | INFO     | __main__:main:235 - max_length                     256
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - optimizer                      adamw
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - lr                             0.006
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - scheduler                      cosine
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - min_lr_ratio                   0.1
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - activation_checkpointing       False
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - weight_decay                   0.0
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - warmup_steps                   1000
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - eval_every                     1000
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - num_training_steps             10000
2025-04-01 15:11:29.908 | INFO     | __main__:main:235 - max_train_tokens               None
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - save_every                     10000
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - save_dir                       checkpoints/llama_60m-2025-04-01-15-11-28
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - tags                           None
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - dtype                          bfloat16
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - workers                        8
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - seed                           42
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - grad_clipping                  0.0
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - beta1                          0.0
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - single_gpu                     False
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - rank                           128
2025-04-01 15:11:29.909 | INFO     | __main__:main:235 - update_proj_gap                50
2025-04-01 15:11:29.910 | INFO     | __main__:main:235 - galore_scale                   1.0
2025-04-01 15:11:29.910 | INFO     | __main__:main:235 - proj_type                      std
2025-04-01 15:11:29.910 | INFO     | __main__:main:236 - ****************************************
2025-04-01 15:11:30.548 | INFO     | __main__:main:242 - Shuffling data with seed 42
2025-04-01 15:14:10.999 | INFO     | __main__:main:304 - Training from scratch, global step will start from zero
2025-04-01 15:14:11.000 | INFO     | __main__:main:307 - ****************************************
/home/wt/miniconda3/envs/moc/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
2025-04-01 15:14:12.039 | INFO     | __main__:main:410 - Running with moc

2025-04-01 15:14:12.040 | INFO     | __main__:main:411 -
LlamaForCausalLM_MoC(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=31999)
    (layers): ModuleList(
      (0-7): 8 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=512, out_features=512, bias=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=False)
          (v_proj): Linear(in_features=512, out_features=512, bias=False)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMoC_triton(
          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)
          (down_proj): Linear(in_features=1376, out_features=512, bias=False)
          (up_proj): Linear(in_features=512, out_features=1376, bias=False)
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)

2025-04-01 15:14:12.040 | INFO     | __main__:main:412 - All params:
['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight']

2025-04-01 15:14:12.041 | INFO     | __main__:main:415 - Total params: 58.07M
2025-04-01 15:14:12.041 | INFO     | __main__:main:418 - Total non-low-rank parameters: 58.07M
2025-04-01 15:14:12.041 | INFO     | __main__:main:427 - Trainable params: 58.07M
2025-04-01 15:14:12.041 | INFO     | __main__:main:430 - Saving model to checkpoints/llama_60m-2025-04-01-15-11-28 every 10000 update steps
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/10000 [00:00<?, ?it/s]2025-04-01 15:14:12.243 | INFO     | __main__:main:472 - Maximum memory allocated before training: 238065664 bytes

Update steps:  10%|██▏                   | 1000/10000 [13:00<2:18:54,  1.08it/s]2025-04-01 15:27:12.938 | INFO     | __main__:main:572 - Performing evaluation at step 1000
Traceback (most recent call last):
  File "/home/wt/moc_pretrain/main.py", line 704, in <module>
    main(args)
  File "/home/wt/moc_pretrain/main.py", line 574, in main
    total_loss, evaluated_on_tokens = evaluate_model(
                                      ^^^^^^^^^^^^^^^
  File "/home/wt/miniconda3/envs/moc/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
TypeError: evaluate_model() takes 7 positional arguments but 8 were given
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/wt/moc_pretrain/main.py", line 704, in <module>
[rank0]:     main(args)
[rank0]:   File "/home/wt/moc_pretrain/main.py", line 574, in main
[rank0]:     total_loss, evaluated_on_tokens = evaluate_model(
[rank0]:                                       ^^^^^^^^^^^^^^^
[rank0]:   File "/home/wt/miniconda3/envs/moc/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]: TypeError: evaluate_model() takes 7 positional arguments but 8 were given
