2025-04-01 15:01:43.002 | INFO     | __main__:main:231 - Using dist with rank 0 (only rank 0 will log)
2025-04-01 15:01:43.002 | INFO     | __main__:main:232 - ****************************************
2025-04-01 15:01:43.002 | INFO     | __main__:main:233 - Starting training with the arguments
2025-04-01 15:01:43.002 | INFO     | __main__:main:235 - model_type                     moc
2025-04-01 15:01:43.002 | INFO     | __main__:main:235 - run_name                       moc_60m-LR-0.006
2025-04-01 15:01:43.002 | INFO     | __main__:main:235 - wandb_project                  moc_pretrain
2025-04-01 15:01:43.002 | INFO     | __main__:main:235 - model_config                   configs/llama/llama_60m.json
2025-04-01 15:01:43.002 | INFO     | __main__:main:235 - continue_from                  None
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - batch_size                     256
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - gradient_accumulation          2
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - total_batch_size               512
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - max_length                     256
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - optimizer                      adamw
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - lr                             0.006
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - scheduler                      cosine
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - min_lr_ratio                   0.1
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - activation_checkpointing       False
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - weight_decay                   0.0
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - warmup_steps                   1000
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - eval_every                     1000
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - num_training_steps             10000
2025-04-01 15:01:43.003 | INFO     | __main__:main:235 - max_train_tokens               None
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - save_every                     10000
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - save_dir                       checkpoints/llama_60m-2025-04-01-15-01-41
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - tags                           None
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - dtype                          bfloat16
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - workers                        8
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - seed                           42
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - grad_clipping                  0.0
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - beta1                          0.0
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - single_gpu                     False
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - rank                           128
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - update_proj_gap                50
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - galore_scale                   1.0
2025-04-01 15:01:43.004 | INFO     | __main__:main:235 - proj_type                      std
2025-04-01 15:01:43.004 | INFO     | __main__:main:236 - ****************************************
2025-04-01 15:01:43.610 | INFO     | __main__:main:242 - Shuffling data with seed 42
wandb: WARNING Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.
2025-04-01 15:04:24.076 | INFO     | __main__:main:304 - Training from scratch, global step will start from zero
2025-04-01 15:04:24.076 | INFO     | __main__:main:307 - ****************************************
/home/wt/miniconda3/envs/moc/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
2025-04-01 15:04:25.065 | INFO     | __main__:main:410 - Running with moc

2025-04-01 15:04:25.066 | INFO     | __main__:main:411 -
LlamaForCausalLM_MoC(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=31999)
    (layers): ModuleList(
      (0-7): 8 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=512, out_features=512, bias=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=False)
          (v_proj): Linear(in_features=512, out_features=512, bias=False)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMoC_triton(
          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)
          (down_proj): Linear(in_features=1376, out_features=512, bias=False)
          (up_proj): Linear(in_features=512, out_features=1376, bias=False)
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)

2025-04-01 15:04:25.066 | INFO     | __main__:main:412 - All params:
['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight']

2025-04-01 15:04:25.066 | INFO     | __main__:main:415 - Total params: 58.07M
2025-04-01 15:04:25.067 | INFO     | __main__:main:418 - Total non-low-rank parameters: 58.07M
2025-04-01 15:04:25.067 | INFO     | __main__:main:427 - Trainable params: 58.07M
2025-04-01 15:04:25.067 | INFO     | __main__:main:430 - Saving model to checkpoints/llama_60m-2025-04-01-15-01-41 every 10000 update steps
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
