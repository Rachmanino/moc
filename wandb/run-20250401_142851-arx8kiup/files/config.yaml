_wandb:
    value:
        cli_version: 0.19.8
        m: []
        python_version: 3.12.0
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 55
            "2":
                - 1
                - 11
                - 49
                - 51
                - 55
            "3":
                - 3
                - 13
                - 23
                - 55
                - 61
            "4": 3.12.0
            "5": 0.19.8
            "6": 4.50.2
            "8":
                - 5
            "12": 0.19.8
            "13": linux-x86_64
activation_checkpointing:
    value: false
batch_size:
    value: 256
beta1:
    value: 0
continue_from:
    value: null
dataset:
    value: allenai/c4
device:
    value: cuda:0
dtype:
    value: bfloat16
eval_every:
    value: 1000
galore_scale:
    value: 1
grad_clipping:
    value: 0
gradient_accumulation:
    value: 2
max_length:
    value: 256
max_lr:
    value: 0.006
max_train_tokens:
    value: null
min_lr_ratio:
    value: 0.1
model:
    value:
        _attn_implementation_autoset: true
        _name_or_path: ""
        act_channels: 256
        add_cross_attention: false
        architectures:
            - LLaMAForCausalLM
        attention_bias: false
        attention_dropout: 0
        bad_words_ids: null
        begin_suppress_tokens: null
        bos_token_id: 0
        chunk_size_feed_forward: 0
        cross_attention_hidden_size: null
        decoder_start_token_id: null
        diversity_penalty: 0
        do_sample: false
        early_stopping: false
        encoder_no_repeat_ngram_size: 0
        eos_token_id: 1
        exponential_decay_length_penalty: null
        finetuning_task: null
        forced_bos_token_id: null
        forced_eos_token_id: null
        head_dim: 64
        hidden_act: silu
        hidden_size: 512
        id2label:
            "0": LABEL_0
            "1": LABEL_1
        initializer_range: 0.02
        intermediate_size: 1376
        is_decoder: false
        is_encoder_decoder: false
        label2id:
            LABEL_0: 0
            LABEL_1: 1
        length_penalty: 1
        max_length: 20
        max_position_embeddings: 2048
        max_sequence_length: 1024
        min_length: 0
        mlp_bias: false
        model_type: llama
        no_repeat_ngram_size: 0
        num_attention_heads: 8
        num_beam_groups: 1
        num_beams: 1
        num_hidden_layers: 8
        num_key_value_heads: 8
        num_return_sequences: 1
        output_attentions: false
        output_hidden_states: false
        output_scores: false
        pad_token_id: -1
        prefix: null
        pretraining_tp: 1
        problem_type: null
        remove_invalid_values: false
        repetition_penalty: 1
        return_dict: true
        return_dict_in_generate: false
        rms_norm_eps: 1e-06
        rope_scaling: null
        rope_theta: 10000
        sep_token_id: null
        suppress_tokens: null
        task_specific_params: null
        temperature: 1
        tf_legacy_loss: false
        tie_encoder_decoder: false
        tie_word_embeddings: false
        tokenizer_class: null
        top_k: 50
        top_p: 1
        torch_dtype: null
        torchscript: false
        transformers_version: 4.50.2
        typical_p: 1
        use_bfloat16: false
        use_cache: false
        vocab_size: 32000
model_config:
    value: configs/llama/llama_60m.json
model_type:
    value: moc
num_training_steps:
    value: 10000
optimizer:
    value: adamw
proj_type:
    value: std
rank:
    value: 128
run_name:
    value: moc_60m-LR-0.006
save_dir:
    value: checkpoints/llama_60m-2025-04-01-14-28-50
save_every:
    value: 10000
scheduler:
    value: cosine
seed:
    value: 42
single_gpu:
    value: false
tags:
    value: null
total_batch_size:
    value: 512
total_params_M:
    value: 58.0736
update_proj_gap:
    value: 50
wandb_project:
    value: moc_pretrain
warmup_steps:
    value: 1000
weight_decay:
    value: 0
workers:
    value: 8
world_size:
    value: 1
