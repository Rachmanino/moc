{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from moc_kernel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, s, d, e, c = 256, 512, 768, 2048, 384 # for benchmarking\n",
    "# b, s, d, e, c = 2, 2, 3, 4, 2 # for debugging\n",
    "device = 'cuda' # Use cuda:0!!!!!\n",
    "dtype = torch.bfloat16 # amp requires bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module): # Not memory-efficient\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        hidden_act: str,\n",
    "        act_channels: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "        self.act_channels = act_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        return self.down_proj(self.act_fn(gate) * self.up_proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm         1.89%     571.629us         2.21%     667.419us     111.236us      16.188ms        51.37%      16.188ms       2.698ms             6  \n",
      "autograd::engine::evaluate_function: FusedSparseSwiG...         0.35%     106.997us         7.51%       2.274ms       2.274ms       4.000us         0.01%      11.048ms      11.048ms             1  \n",
      "                              FusedSparseSwiGLUBackward         2.41%     729.659us         7.13%       2.156ms       2.156ms       3.356ms        10.65%      11.044ms      11.044ms             1  \n",
      "                                           aten::matmul         0.42%     127.223us         3.47%       1.050ms     262.545us      64.000us         0.20%      10.778ms       2.695ms             4  \n",
      "                                      FusedSparseSwiGLU         2.19%     664.023us         5.50%       1.664ms       1.664ms       1.860ms         5.90%       8.993ms       8.993ms             1  \n",
      "                                           aten::linear         0.17%      51.576us         2.66%     805.329us     402.664us      54.000us         0.17%       5.788ms       2.894ms             2  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.10%      30.558us         1.14%     344.757us     172.379us       7.000us         0.02%       5.594ms       2.797ms             2  \n",
      "                                            MmBackward0         0.16%      47.929us         0.99%     300.162us     150.081us      16.000us         0.05%       5.587ms       2.793ms             2  \n",
      "                                             aten::topk         0.59%     177.548us         1.54%     466.628us     466.628us       3.412ms        10.83%       3.412ms       3.412ms             1  \n",
      "                                           aten::einsum         0.41%     122.789us         1.75%     530.909us     530.909us      28.000us         0.09%       2.847ms       2.847ms             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 30.255ms\n",
      "Self CUDA time total: 31.514ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LlamaMoC_triton(d, e, 'silu', c).to(device, dtype)\n",
    "x = torch.randn(b, s, d, device=device, dtype=dtype)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_device='cuda') as prof:\n",
    "   y = model(x)\n",
    "   y.backward(y)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sltrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
